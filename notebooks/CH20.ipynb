{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anaconda에서 개발환경 만들기\n",
    "```bash\n",
    "conda create -n book-trasnfer-learning python=3.5\n",
    "conda activate book-transfer-learning\n",
    "conda install jupyter notebook tqdm tensorflow scikit-learn matplotlib scikit-image\n",
    "git clone https://github.com/machrisaa/tensorflow-vgg tensorflow_vgg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tarfile\n",
    "dataset_folder_path = 'flower_photos'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('flower_photos.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Flowers Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "            'flower_photos.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(dataset_folder_path):\n",
    "    with tarfile.open('flower_photos.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "with open('labels.txt') as f:\n",
    "    reader = csv.reader(f, delimiter='\\n')\n",
    "    labels = np.array([each for each in reader if len(each) > 0]).squeeze()\n",
    "\n",
    "with open('codes.bin') as f:\n",
    "    codes = np.fromfile(f, dtype=np.float32)\n",
    "    codes = codes.reshape((len(labels), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3670, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(codes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['daisy' 'dandelion' 'roses' 'sunflowers' 'tulips']\n",
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(labels)\n",
    "\n",
    "labels_vecs = lb.transform(labels)\n",
    "\n",
    "print(labels[[0, 1000, 2000, 2500, 3000]])\n",
    "print(labels_vecs[[0, 1000, 2000, 2500, 3000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes (x, y): (2936, 4096) (2936, 5)\n",
      "Validation shapes (x, y): (367, 4096) (367, 5)\n",
      "Test shapes (x, y): (367, 4096) (367, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "train_idx, val_idx = next(ss.split(codes, labels_vecs))\n",
    "\n",
    "half_val_len = int(len(val_idx)/2)\n",
    "val_idx, test_idx = val_idx[:half_val_len], val_idx[half_val_len:]\n",
    "\n",
    "train_x, train_y = codes[train_idx], labels_vecs[train_idx]\n",
    "val_x, val_y = codes[val_idx], labels_vecs[val_idx]\n",
    "test_x, test_y = codes[test_idx], labels_vecs[test_idx]\n",
    "\n",
    "print(\"Train shapes (x, y):\", train_x.shape, train_y.shape)\n",
    "print(\"Validation shapes (x, y):\", val_x.shape, val_y.shape)\n",
    "print(\"Test shapes (x, y):\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense/Relu:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"dense_1/BiasAdd:0\", shape=(?, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inputs_ = tf.placeholder(tf.float32, shape=[None, codes.shape[1]])\n",
    "labels_ = tf.placeholder(tf.float32, shape=[None, labels_vecs.shape[1]])\n",
    "\n",
    "fc = tf.layers.dense(inputs_, 256, activation=tf.nn.relu)\n",
    "print(fc)\n",
    "logits = tf.layers.dense(fc, labels_vecs.shape[1], activation=None)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-7bc07355f6a3>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels_, logits=logits)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "predicted = tf.nn.softmax(logits)\n",
    "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 10\n",
    "\n",
    "def get_batches(x, y, batch_size=16):\n",
    "    \"\"\" Return a generator that yields batches from arrays x and y. \"\"\"\n",
    "    n_batches = np.int32(np.floor(len(x) / batch_size))\n",
    "    \n",
    "    for ii in range(0, n_batches*batch_size, batch_size):\n",
    "        # If we're not on the last batch, grab data with size batch_size\n",
    "        if ii != (n_batches-1)*batch_size:\n",
    "            X, Y = x[ii: ii+batch_size], y[ii: ii+batch_size] \n",
    "        # On the last batch, grab the rest of the data\n",
    "        else:\n",
    "            X, Y = x[ii:], y[ii:]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory 'checkpoints': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Validation Acc: 0.8501\n",
      "Epoch: 2/10 Validation Acc: 0.8501\n",
      "Epoch: 3/10 Validation Acc: 0.8501\n",
      "Epoch: 4/10 Validation Acc: 0.8420\n",
      "Epoch: 5/10 Validation Acc: 0.8638\n",
      "Epoch: 6/10 Validation Acc: 0.8828\n",
      "Epoch: 7/10 Validation Acc: 0.8610\n",
      "Epoch: 8/10 Validation Acc: 0.8556\n",
      "Epoch: 9/10 Validation Acc: 0.8665\n",
      "Epoch: 10/10 Validation Acc: 0.8883\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for x, y in get_batches(train_x, train_y, batch_size):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y}\n",
    "            loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "        feed = {inputs_: val_x,\n",
    "                labels_: val_y}\n",
    "        val_acc = sess.run(accuracy, feed_dict=feed)\n",
    "        print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"Validation Acc: {:.4f}\".format(val_acc))\n",
    "    saver.save(sess, \"checkpoints/flowers.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/flowers.ckpt\n",
      "Test accuracy: 0.9074\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    feed = {inputs_: test_x,\n",
    "            labels_: test_y}\n",
    "    test_acc = sess.run(accuracy, feed_dict=feed)\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottlne 특성 추출 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roses', 'sunflowers', 'dandelion', 'daisy', 'tulips']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "data_dir = 'flower_photos/'\n",
    "contents = os.listdir(data_dir)\n",
    "classes = [each for each in contents if os.path.isdir(data_dir + each)]\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VGG16 Parameters: 553MB [01:15, 7.32MB/s]                               \n"
     ]
    }
   ],
   "source": [
    "vgg_dir = 'tensorflow_vgg/'\n",
    "if not isdir(vgg_dir):\n",
    "    raise Exception(\"VGG directory doesn't exist!\")\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(vgg_dir + \"vgg16.npy\"):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='VGG16 Parameters') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://s3.amazonaws.com/content.udacity-data.com/nd101/vgg16.npy',\n",
    "            vgg_dir + 'vgg16.npy',\n",
    "            pbar.hook)\n",
    "else:\n",
    "    print(\"Parameter file already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yhbyhb/Documents/GitHub/book2/codes/tensorflow_vgg/vgg16.npy\n",
      "npy file loaded\n",
      "build model started\n",
      "build model finished: 2s\n",
      "Starting roses images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yhbyhb/anaconda3/envs/book-transfer-learning/lib/python3.5/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/yhbyhb/anaconda3/envs/book-transfer-learning/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 images processed\n",
      "32 images processed\n",
      "48 images processed\n",
      "64 images processed\n",
      "80 images processed\n",
      "96 images processed\n",
      "112 images processed\n",
      "128 images processed\n",
      "144 images processed\n",
      "160 images processed\n",
      "176 images processed\n",
      "192 images processed\n",
      "208 images processed\n",
      "224 images processed\n",
      "240 images processed\n",
      "256 images processed\n",
      "272 images processed\n",
      "288 images processed\n",
      "304 images processed\n",
      "320 images processed\n",
      "336 images processed\n",
      "352 images processed\n",
      "368 images processed\n",
      "384 images processed\n",
      "400 images processed\n",
      "416 images processed\n",
      "432 images processed\n",
      "448 images processed\n",
      "464 images processed\n",
      "480 images processed\n",
      "496 images processed\n",
      "512 images processed\n",
      "528 images processed\n",
      "544 images processed\n",
      "560 images processed\n",
      "576 images processed\n",
      "592 images processed\n",
      "608 images processed\n",
      "624 images processed\n",
      "640 images processed\n",
      "641 images processed\n",
      "Starting sunflowers images\n",
      "16 images processed\n",
      "32 images processed\n",
      "48 images processed\n",
      "64 images processed\n",
      "80 images processed\n",
      "96 images processed\n",
      "112 images processed\n",
      "128 images processed\n",
      "144 images processed\n",
      "160 images processed\n",
      "176 images processed\n",
      "192 images processed\n",
      "208 images processed\n",
      "224 images processed\n",
      "240 images processed\n",
      "256 images processed\n",
      "272 images processed\n",
      "288 images processed\n",
      "304 images processed\n",
      "320 images processed\n",
      "336 images processed\n",
      "352 images processed\n",
      "368 images processed\n",
      "384 images processed\n",
      "400 images processed\n",
      "416 images processed\n",
      "432 images processed\n",
      "448 images processed\n",
      "464 images processed\n",
      "480 images processed\n",
      "496 images processed\n",
      "512 images processed\n",
      "528 images processed\n",
      "544 images processed\n",
      "560 images processed\n",
      "576 images processed\n",
      "592 images processed\n",
      "608 images processed\n",
      "624 images processed\n",
      "640 images processed\n",
      "656 images processed\n",
      "672 images processed\n",
      "688 images processed\n",
      "699 images processed\n",
      "Starting dandelion images\n",
      "16 images processed\n",
      "32 images processed\n",
      "48 images processed\n",
      "64 images processed\n",
      "80 images processed\n",
      "96 images processed\n",
      "112 images processed\n",
      "128 images processed\n",
      "144 images processed\n",
      "160 images processed\n",
      "176 images processed\n",
      "192 images processed\n",
      "208 images processed\n",
      "224 images processed\n",
      "240 images processed\n",
      "256 images processed\n",
      "272 images processed\n",
      "288 images processed\n",
      "304 images processed\n",
      "320 images processed\n",
      "336 images processed\n",
      "352 images processed\n",
      "368 images processed\n",
      "384 images processed\n",
      "400 images processed\n",
      "416 images processed\n",
      "432 images processed\n",
      "448 images processed\n",
      "464 images processed\n",
      "480 images processed\n",
      "496 images processed\n",
      "512 images processed\n",
      "528 images processed\n",
      "544 images processed\n",
      "560 images processed\n",
      "576 images processed\n",
      "592 images processed\n",
      "608 images processed\n",
      "624 images processed\n",
      "640 images processed\n",
      "656 images processed\n",
      "672 images processed\n",
      "688 images processed\n",
      "704 images processed\n",
      "720 images processed\n",
      "736 images processed\n",
      "752 images processed\n",
      "768 images processed\n",
      "784 images processed\n",
      "800 images processed\n",
      "816 images processed\n",
      "832 images processed\n",
      "848 images processed\n",
      "864 images processed\n",
      "880 images processed\n",
      "896 images processed\n",
      "898 images processed\n",
      "Starting daisy images\n",
      "16 images processed\n",
      "32 images processed\n",
      "48 images processed\n",
      "64 images processed\n",
      "80 images processed\n",
      "96 images processed\n",
      "112 images processed\n",
      "128 images processed\n",
      "144 images processed\n",
      "160 images processed\n",
      "176 images processed\n",
      "192 images processed\n",
      "208 images processed\n",
      "224 images processed\n",
      "240 images processed\n",
      "256 images processed\n",
      "272 images processed\n",
      "288 images processed\n",
      "304 images processed\n",
      "320 images processed\n",
      "336 images processed\n",
      "352 images processed\n",
      "368 images processed\n",
      "384 images processed\n",
      "400 images processed\n",
      "416 images processed\n",
      "432 images processed\n",
      "448 images processed\n",
      "464 images processed\n",
      "480 images processed\n",
      "496 images processed\n",
      "512 images processed\n",
      "528 images processed\n",
      "544 images processed\n",
      "560 images processed\n",
      "576 images processed\n",
      "592 images processed\n",
      "608 images processed\n",
      "624 images processed\n",
      "633 images processed\n",
      "Starting tulips images\n",
      "16 images processed\n",
      "32 images processed\n",
      "48 images processed\n",
      "64 images processed\n",
      "80 images processed\n",
      "96 images processed\n",
      "112 images processed\n",
      "128 images processed\n",
      "144 images processed\n",
      "160 images processed\n",
      "176 images processed\n",
      "192 images processed\n",
      "208 images processed\n",
      "224 images processed\n",
      "240 images processed\n",
      "256 images processed\n",
      "272 images processed\n",
      "288 images processed\n",
      "304 images processed\n",
      "320 images processed\n",
      "336 images processed\n",
      "352 images processed\n",
      "368 images processed\n",
      "384 images processed\n",
      "400 images processed\n",
      "416 images processed\n",
      "432 images processed\n",
      "448 images processed\n",
      "464 images processed\n",
      "480 images processed\n",
      "496 images processed\n",
      "512 images processed\n",
      "528 images processed\n",
      "544 images processed\n",
      "560 images processed\n",
      "576 images processed\n",
      "592 images processed\n",
      "608 images processed\n",
      "624 images processed\n",
      "640 images processed\n",
      "656 images processed\n",
      "672 images processed\n",
      "688 images processed\n",
      "704 images processed\n",
      "720 images processed\n",
      "736 images processed\n",
      "752 images processed\n",
      "768 images processed\n",
      "784 images processed\n",
      "799 images processed\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_vgg import vgg16, utils\n",
    "# Set the batch size higher if you can fit in in your GPU memory\n",
    "batch_size = 16\n",
    "codes_list = []\n",
    "labels = []\n",
    "batch = []\n",
    "\n",
    "codes = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    vgg = vgg16.Vgg16()\n",
    "    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    with tf.name_scope(\"content_vgg\"):\n",
    "        vgg.build(input_)\n",
    "\n",
    "    for each in classes:\n",
    "        print(\"Starting {} images\".format(each))\n",
    "        class_path = data_dir + each\n",
    "        files = os.listdir(class_path)\n",
    "        for ii, file in enumerate(files, 1):\n",
    "            # Add images to the current batch\n",
    "            # utils.load_image crops the input images for us, from the center\n",
    "            img = utils.load_image(os.path.join(class_path, file))\n",
    "            batch.append(img.reshape((1, 224, 224, 3)))\n",
    "            labels.append(each)\n",
    "            \n",
    "            # Running the batch through the network to get the codes\n",
    "            if ii % batch_size == 0 or ii == len(files):\n",
    "                images = np.concatenate(batch)\n",
    "\n",
    "                feed_dict = {input_: images}\n",
    "                codes_batch = sess.run(vgg.relu6, feed_dict=feed_dict)# reshape\n",
    "                \n",
    "                # Here I'm building an array of the codes\n",
    "                if codes is None:\n",
    "                    codes = codes_batch\n",
    "                else:\n",
    "                    codes = np.concatenate((codes, codes_batch))\n",
    "                \n",
    "                # Reset to start building the next batch\n",
    "                batch = []\n",
    "                print('{} images processed'.format(ii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write codes to file\n",
    "with open('codes.bin', 'w') as f:\n",
    "    codes.tofile(f)\n",
    "    \n",
    "# write labels to file\n",
    "import csv\n",
    "with open('labels.txt', 'w') as f:\n",
    "    writer = csv.writer(f, delimiter='\\n')\n",
    "    writer.writerow(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
