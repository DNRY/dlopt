{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20장. 영상"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 20.2.2 환경 준비\n",
    "```bash\n",
    "conda env create -f env_CH20.yml\n",
    "conda activate book-transfer-learning\n",
    "git clone https://github.com/machrisaa/tensorflow-vgg tensorflow_vgg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 20.2.5 데이터 훓어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Flowers Dataset: 229MB [00:06, 33.0MB/s]\n"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tarfile\n",
    "dataset_folder_path = 'flower_photos'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('flower_photos.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Flowers Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "            'flower_photos.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(dataset_folder_path):\n",
    "    with tarfile.open('flower_photos.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "with open('labels.txt') as f:\n",
    "    reader = csv.reader(f, delimiter='\\n')\n",
    "    labels = np.array([each for each in reader if len(each) > 0]).squeeze()\n",
    "\n",
    "with open('codes.bin') as f:\n",
    "    codes = np.fromfile(f, dtype=np.float32)\n",
    "    codes = codes.reshape((len(labels), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(3670, 4096)\n"
    }
   ],
   "source": [
    "print(codes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['daisy' 'dandelion' 'roses' 'sunflowers' 'tulips']\n[[1 0 0 0 0]\n [0 1 0 0 0]\n [0 0 1 0 0]\n [0 0 0 1 0]\n [0 0 0 0 1]]\n"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(labels)\n",
    "\n",
    "labels_vecs = lb.transform(labels)\n",
    "\n",
    "print(labels[[0, 1000, 2000, 2500, 3000]])\n",
    "print(labels_vecs[[0, 1000, 2000, 2500, 3000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train shapes (x, y): (2936, 4096) (2936, 5)\nValidation shapes (x, y): (367, 4096) (367, 5)\nTest shapes (x, y): (367, 4096) (367, 5)\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "train_idx, val_idx = next(ss.split(codes, labels_vecs))\n",
    "\n",
    "half_val_len = int(len(val_idx)/2)\n",
    "val_idx, test_idx = val_idx[:half_val_len], val_idx[half_val_len:]\n",
    "\n",
    "train_x, train_y = codes[train_idx], labels_vecs[train_idx]\n",
    "val_x, val_y = codes[val_idx], labels_vecs[val_idx]\n",
    "test_x, test_y = codes[test_idx], labels_vecs[test_idx]\n",
    "\n",
    "print(\"Train shapes (x, y):\", train_x.shape, train_y.shape)\n",
    "print(\"Validation shapes (x, y):\", val_x.shape, val_y.shape)\n",
    "print(\"Test shapes (x, y):\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 20.2.6 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Tensor(\"dense/Relu:0\", shape=(?, 256), dtype=float32)\nTensor(\"dense_1/BiasAdd:0\", shape=(?, 5), dtype=float32)\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inputs_ = tf.placeholder(tf.float32, shape=[None, codes.shape[1]])\n",
    "labels_ = tf.placeholder(tf.float32, shape=[None, labels_vecs.shape[1]])\n",
    "\n",
    "fc = tf.layers.dense(inputs_, 256, activation=tf.nn.relu)\n",
    "print(fc)\n",
    "logits = tf.layers.dense(fc, labels_vecs.shape[1], activation=None)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 20.2.7 최적화 문제 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "WARNING:tensorflow:From <ipython-input-7-7bc07355f6a3>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n"
    }
   ],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels_, logits=logits)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "predicted = tf.nn.softmax(logits)\n",
    "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 20.2.8 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 10\n",
    "\n",
    "def get_batches(x, y, batch_size=16):\n",
    "    \"\"\" Return a generator that yields batches from arrays x and y. \"\"\"\n",
    "    n_batches = np.int32(np.floor(len(x) / batch_size))\n",
    "    \n",
    "    for ii in range(0, n_batches*batch_size, batch_size):\n",
    "        # If we're not on the last batch, grab data with size batch_size\n",
    "        if ii != (n_batches-1)*batch_size:\n",
    "            X, Y = x[ii: ii+batch_size], y[ii: ii+batch_size] \n",
    "        # On the last batch, grab the rest of the data\n",
    "        else:\n",
    "            X, Y = x[ii:], y[ii:]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 20.2.9 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch: 1/10 Validation Acc: 0.8283\nEpoch: 2/10 Validation Acc: 0.8719\nEpoch: 3/10 Validation Acc: 0.8529\nEpoch: 4/10 Validation Acc: 0.8583\nEpoch: 5/10 Validation Acc: 0.8529\nEpoch: 6/10 Validation Acc: 0.8474\nEpoch: 7/10 Validation Acc: 0.8665\nEpoch: 8/10 Validation Acc: 0.8583\nEpoch: 9/10 Validation Acc: 0.8610\nEpoch: 10/10 Validation Acc: 0.8774\n"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for x, y in get_batches(train_x, train_y, batch_size):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y}\n",
    "            loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "        feed = {inputs_: val_x,\n",
    "                labels_: val_y}\n",
    "        val_acc = sess.run(accuracy, feed_dict=feed)\n",
    "        print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"Validation Acc: {:.4f}\".format(val_acc))\n",
    "    saver.save(sess, \"checkpoints/flowers.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 20.2.10 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "INFO:tensorflow:Restoring parameters from checkpoints\\flowers.ckpt\nTest accuracy: 0.9101\n"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    feed = {inputs_: test_x,\n",
    "            labels_: test_y}\n",
    "    test_acc = sess.run(accuracy, feed_dict=feed)\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 20.3 Bottlne 특성 추출 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "data_dir = 'flower_photos/'\n",
    "contents = os.listdir(data_dir)\n",
    "classes = [each for each in contents if os.path.isdir(data_dir + each)]\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "VGG16 Parameters: 553MB [22:35, 408kB/s]\n"
    }
   ],
   "source": [
    "vgg_dir = 'tensorflow_vgg/'\n",
    "if not isdir(vgg_dir):\n",
    "    raise Exception(\"VGG directory doesn't exist!\")\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(vgg_dir + \"vgg16.npy\"):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='VGG16 Parameters') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://s3.amazonaws.com/content.udacity-data.com/nd101/vgg16.npy',\n",
    "            vgg_dir + 'vgg16.npy',\n",
    "            pbar.hook)\n",
    "else:\n",
    "    print(\"Parameter file already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "e:\\repos\\tmp\\dlopt\\notebooks\\tensorflow_vgg\\vgg16.npy\nnpy file loaded\nbuild model started\nbuild model finished: 1s\nStarting daisy images\nC:\\Users\\HanByul\\Anaconda3\\envs\\book-trasnfer-learning\\lib\\site-packages\\skimage\\transform\\_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\nC:\\Users\\HanByul\\Anaconda3\\envs\\book-trasnfer-learning\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n16 images processed\n32 images processed\n48 images processed\n64 images processed\n80 images processed\n96 images processed\n112 images processed\n128 images processed\n144 images processed\n160 images processed\n176 images processed\n192 images processed\n208 images processed\n224 images processed\n240 images processed\n256 images processed\n272 images processed\n288 images processed\n304 images processed\n320 images processed\n336 images processed\n352 images processed\n368 images processed\n384 images processed\n400 images processed\n416 images processed\n432 images processed\n448 images processed\n464 images processed\n480 images processed\n496 images processed\n512 images processed\n528 images processed\n544 images processed\n560 images processed\n576 images processed\n592 images processed\n608 images processed\n624 images processed\n633 images processed\nStarting dandelion images\n16 images processed\n32 images processed\n48 images processed\n64 images processed\n80 images processed\n96 images processed\n112 images processed\n128 images processed\n144 images processed\n160 images processed\n176 images processed\n192 images processed\n208 images processed\n224 images processed\n240 images processed\n256 images processed\n272 images processed\n288 images processed\n304 images processed\n320 images processed\n336 images processed\n352 images processed\n368 images processed\n384 images processed\n400 images processed\n416 images processed\n432 images processed\n448 images processed\n464 images processed\n480 images processed\n496 images processed\n512 images processed\n528 images processed\n544 images processed\n560 images processed\n576 images processed\n592 images processed\n608 images processed\n624 images processed\n640 images processed\n656 images processed\n672 images processed\n688 images processed\n704 images processed\n720 images processed\n736 images processed\n752 images processed\n768 images processed\n784 images processed\n800 images processed\n816 images processed\n832 images processed\n848 images processed\n864 images processed\n880 images processed\n896 images processed\n898 images processed\nStarting roses images\n16 images processed\n32 images processed\n48 images processed\n64 images processed\n80 images processed\n96 images processed\n112 images processed\n128 images processed\n144 images processed\n160 images processed\n176 images processed\n192 images processed\n208 images processed\n224 images processed\n240 images processed\n256 images processed\n272 images processed\n288 images processed\n304 images processed\n320 images processed\n336 images processed\n352 images processed\n368 images processed\n384 images processed\n400 images processed\n416 images processed\n432 images processed\n448 images processed\n464 images processed\n480 images processed\n496 images processed\n512 images processed\n528 images processed\n544 images processed\n560 images processed\n576 images processed\n592 images processed\n608 images processed\n624 images processed\n640 images processed\n641 images processed\nStarting sunflowers images\n16 images processed\n32 images processed\n48 images processed\n64 images processed\n80 images processed\n96 images processed\n112 images processed\n128 images processed\n144 images processed\n160 images processed\n176 images processed\n192 images processed\n208 images processed\n224 images processed\n240 images processed\n256 images processed\n272 images processed\n288 images processed\n304 images processed\n320 images processed\n336 images processed\n352 images processed\n368 images processed\n384 images processed\n400 images processed\n416 images processed\n432 images processed\n448 images processed\n464 images processed\n480 images processed\n496 images processed\n512 images processed\n528 images processed\n544 images processed\n560 images processed\n576 images processed\n592 images processed\n608 images processed\n624 images processed\n640 images processed\n656 images processed\n672 images processed\n688 images processed\n699 images processed\nStarting tulips images\n16 images processed\n32 images processed\n48 images processed\n64 images processed\n80 images processed\n96 images processed\n112 images processed\n128 images processed\n144 images processed\n160 images processed\n176 images processed\n192 images processed\n208 images processed\n224 images processed\n240 images processed\n256 images processed\n272 images processed\n288 images processed\n304 images processed\n320 images processed\n336 images processed\n352 images processed\n368 images processed\n384 images processed\n400 images processed\n416 images processed\n432 images processed\n448 images processed\n464 images processed\n480 images processed\n496 images processed\n512 images processed\n528 images processed\n544 images processed\n560 images processed\n576 images processed\n592 images processed\n608 images processed\n624 images processed\n640 images processed\n656 images processed\n672 images processed\n688 images processed\n704 images processed\n720 images processed\n736 images processed\n752 images processed\n768 images processed\n784 images processed\n799 images processed\n"
    }
   ],
   "source": [
    "from tensorflow_vgg import vgg16, utils\n",
    "# Set the batch size higher if you can fit in in your GPU memory\n",
    "batch_size = 16\n",
    "codes_list = []\n",
    "labels = []\n",
    "batch = []\n",
    "\n",
    "codes = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    vgg = vgg16.Vgg16()\n",
    "    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    with tf.name_scope(\"content_vgg\"):\n",
    "        vgg.build(input_)\n",
    "\n",
    "    for each in classes:\n",
    "        print(\"Starting {} images\".format(each))\n",
    "        class_path = data_dir + each\n",
    "        files = os.listdir(class_path)\n",
    "        for ii, file in enumerate(files, 1):\n",
    "            # Add images to the current batch\n",
    "            # utils.load_image crops the input images for us, from the center\n",
    "            img = utils.load_image(os.path.join(class_path, file))\n",
    "            batch.append(img.reshape((1, 224, 224, 3)))\n",
    "            labels.append(each)\n",
    "            \n",
    "            # Running the batch through the network to get the codes\n",
    "            if ii % batch_size == 0 or ii == len(files):\n",
    "                images = np.concatenate(batch)\n",
    "\n",
    "                feed_dict = {input_: images}\n",
    "                codes_batch = sess.run(vgg.relu6, feed_dict=feed_dict)# reshape\n",
    "                \n",
    "                # Here I'm building an array of the codes\n",
    "                if codes is None:\n",
    "                    codes = codes_batch\n",
    "                else:\n",
    "                    codes = np.concatenate((codes, codes_batch))\n",
    "                \n",
    "                # Reset to start building the next batch\n",
    "                batch = []\n",
    "                print('{} images processed'.format(ii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write codes to file\n",
    "with open('codes.bin', 'w') as f:\n",
    "    codes.tofile(f)\n",
    "    \n",
    "# write labels to file\n",
    "import csv\n",
    "with open('labels.txt', 'w') as f:\n",
    "    writer = csv.writer(f, delimiter='\\n')\n",
    "    writer.writerow(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}